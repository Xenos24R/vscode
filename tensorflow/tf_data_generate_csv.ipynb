{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python38164bit909497d13ec443c9ad1939dfb1e78050",
   "display_name": "Python 3.8.1 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2.2.0-rc2\n"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing  = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(11610, 8) (11610,)\n(5160, 8) (5160,)\n(3870, 8) (3870,)\n"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train_all,x_test,y_train_all,y_test = train_test_split(\n",
    "    housing.data,housing.target,random_state = 7\n",
    ")\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(\n",
    "    x_train_all,y_train_all,random_state = 11\n",
    ")\n",
    "print(x_train.shape,y_train.shape)\n",
    "print(x_test.shape,y_test.shape)\n",
    "print(x_valid.shape,y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "x_valid_scaled = scaler.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"generate_csv\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "def save_to_csv(output_dir,data,name_prefix,header=None,n_parts=10):\n",
    "    path_format = os.path.join(output_dir,\"{}_{:02d}.csv\")\n",
    "    filenames = []\n",
    "    for file_idx,row_indices in enumerate(np.array_split(np.arange(len(data)),n_parts)):\n",
    "        part_csv = path_format.format(name_prefix,file_idx)\n",
    "        filenames.append(part_csv)\n",
    "        with open(part_csv,\"wt\",encoding=\"utf-8\") as f:\n",
    "            if header is not None:\n",
    "                f.write(header + \"\\n\")\n",
    "            for row_index in row_indices:\n",
    "                f.write(\",\".join([repr(col) for col in data[row_index]]))\n",
    "                f.write('\\n')\n",
    "\n",
    "    return filenames\n",
    "\n",
    "train_data = np.c_[x_train_scaled,y_train]\n",
    "valid_data = np.c_[x_valid_scaled,y_valid]\n",
    "test_data = np.c_[x_test_scaled,y_test]\n",
    "header_cols = housing.feature_names + [\"MidianHouseValue\"]\n",
    "header_str = \",\".join(header_cols)\n",
    "\n",
    "train_filenames = save_to_csv(output_dir,train_data,\"train\",header_str,n_parts=20)\n",
    "valid_filenames = save_to_csv(output_dir,valid_data,\"valid\",header_str,n_parts=10)\n",
    "test_filenames = save_to_csv(output_dir,test_data,\"test\",header_str,n_parts=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "train_filenames\n['generate_csv\\\\train_00.csv',\n 'generate_csv\\\\train_01.csv',\n 'generate_csv\\\\train_02.csv',\n 'generate_csv\\\\train_03.csv',\n 'generate_csv\\\\train_04.csv',\n 'generate_csv\\\\train_05.csv',\n 'generate_csv\\\\train_06.csv',\n 'generate_csv\\\\train_07.csv',\n 'generate_csv\\\\train_08.csv',\n 'generate_csv\\\\train_09.csv',\n 'generate_csv\\\\train_10.csv',\n 'generate_csv\\\\train_11.csv',\n 'generate_csv\\\\train_12.csv',\n 'generate_csv\\\\train_13.csv',\n 'generate_csv\\\\train_14.csv',\n 'generate_csv\\\\train_15.csv',\n 'generate_csv\\\\train_16.csv',\n 'generate_csv\\\\train_17.csv',\n 'generate_csv\\\\train_18.csv',\n 'generate_csv\\\\train_19.csv']\nvalid_filenames\n['generate_csv\\\\valid_00.csv',\n 'generate_csv\\\\valid_01.csv',\n 'generate_csv\\\\valid_02.csv',\n 'generate_csv\\\\valid_03.csv',\n 'generate_csv\\\\valid_04.csv',\n 'generate_csv\\\\valid_05.csv',\n 'generate_csv\\\\valid_06.csv',\n 'generate_csv\\\\valid_07.csv',\n 'generate_csv\\\\valid_08.csv',\n 'generate_csv\\\\valid_09.csv']\ntest_filenames\n['generate_csv\\\\test_00.csv',\n 'generate_csv\\\\test_01.csv',\n 'generate_csv\\\\test_02.csv',\n 'generate_csv\\\\test_03.csv',\n 'generate_csv\\\\test_04.csv',\n 'generate_csv\\\\test_05.csv',\n 'generate_csv\\\\test_06.csv',\n 'generate_csv\\\\test_07.csv',\n 'generate_csv\\\\test_08.csv',\n 'generate_csv\\\\test_09.csv']\n"
    }
   ],
   "source": [
    "import pprint\n",
    "print(\"train_filenames\")\n",
    "pprint.pprint(train_filenames)\n",
    "print(\"valid_filenames\")\n",
    "pprint.pprint(valid_filenames)\n",
    "print(\"test_filenames\")\n",
    "pprint.pprint(test_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(b'generate_csv\\\\train_12.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_05.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_18.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_02.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_11.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_06.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_16.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_13.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_01.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_17.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_08.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_00.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_04.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_09.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_15.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_03.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_19.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_07.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_10.csv', shape=(), dtype=string)\ntf.Tensor(b'generate_csv\\\\train_14.csv', shape=(), dtype=string)\n"
    }
   ],
   "source": [
    "# 1. filename -> dataset\n",
    "#2. read file -> dataset -> datasets ->merge\n",
    "\n",
    "filename_dataset = tf.data.Dataset.list_files(train_filenames)\n",
    "for filename in filename_dataset:\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "b'MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MidianHouseValue'\nb'MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MidianHouseValue'\nb'MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MidianHouseValue'\nb'MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MidianHouseValue'\nb'MedInc,HouseAge,AveRooms,AveBedrms,Population,AveOccup,Latitude,Longitude,MidianHouseValue'\nb'-0.32652634129448693,0.43236189741438374,-0.09345459539684739,-0.08402991822890092,0.8460035745154013,-0.0266316482653991,-0.5617679242614233,0.1422875991184281,2.431'\nb'0.04971034572063198,-0.8492418886278699,-0.06214699417830008,0.17878747064657746,-0.8025354230744277,0.0005066066922077538,0.6466457006743215,-1.1060793768010604,2.286'\nb'-0.8219588176953616,1.874166156711919,0.18212349433218608,-0.03170019246279883,-0.6011178900722581,-0.14337494105109344,1.0852205298015787,-0.8613994495208361,1.054'\nb'0.401276648075221,-0.9293421252555106,-0.05333050451405854,-0.1865945262276826,0.6545661895448709,0.026434465728210874,0.9312527706398824,-1.4406417263474771,2.512'\nb'0.09734603446040174,0.7527628439249472,-0.20218964416999152,-0.1954700015215477,-0.4060513603629498,0.006785531677655949,-0.813715166526018,0.656614793197258,1.119'\nb'2.2754266257529974,-1.249743071766074,1.0294788075585177,-0.17124431895714504,-0.45413752815175606,0.10527151658164971,-0.9023632702857819,0.9012947204774823,3.798'\nb'-1.453851024367546,1.874166156711919,-1.1315714708271856,0.3611276016530489,-0.3978857847006997,-0.03273859332533962,-0.7390641317809511,0.646627857389904,1.875'\nb'-0.46794146200516895,-0.9293421252555106,0.11909925912590703,-0.060470113038678074,0.30344643606811583,-0.021851890609536125,1.873722084296329,-1.0411642940532422,1.012'\nb'-0.8757754235423053,1.874166156711919,-0.9487499555702599,-0.09657184824705009,-0.7163432355284542,-0.07790191228558485,0.9825753570271144,-1.4206678547327694,2.75'\nb'-1.4803330571456954,-0.6890414153725881,-0.35624704887282904,-0.1725588908792445,-0.8215884329530113,-0.1382309124854157,1.9157132913404298,-1.0211904224385344,0.928'\n"
    }
   ],
   "source": [
    "n_readers = 5\n",
    "dataset = filename_dataset.interleave(\n",
    "    lambda filename: tf.data.TextLineDataset(filename),\n",
    "    cycle_length = n_readers\n",
    ")\n",
    "for line in dataset.take(15):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=float32, numpy=3.0>, <tf.Tensor: shape=(), dtype=string, numpy=b'4'>, <tf.Tensor: shape=(), dtype=float32, numpy=5.0>]\n"
    }
   ],
   "source": [
    "# tf.io.decode_csv(str,record_defaults)\n",
    "\n",
    "sample_str = '1,2,3,4,5'\n",
    "#record_defaults = [tf.constant(0,dtype=tf.int32)] * 5\n",
    "record_defaults = [\n",
    "    tf.constant(0,dtype=tf.int32),\n",
    "    0,\n",
    "    np.nan,\n",
    "    \"hello\",\n",
    "    tf.constant([])\n",
    "]\n",
    "parsed_fields = tf.io.decode_csv(sample_str,record_defaults)\n",
    "print(parsed_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Field 4 is required but missing in record 0! [Op:DecodeCSV]\n"
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv(',,,,',record_defaults)\n",
    "except tf.errors.InvalidArgumentError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Expect 5 fields but have 6 in record 0 [Op:DecodeCSV]\n"
    }
   ],
   "source": [
    "try:\n",
    "    parsed_fields = tf.io.decode_csv('1,2,3,4,5,6',record_defaults)\n",
    "except tf.errors.InvalidArgumentError as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}